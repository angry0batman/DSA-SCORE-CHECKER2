{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basics</th>\n",
       "      <th>stl</th>\n",
       "      <th>sorting</th>\n",
       "      <th>searching</th>\n",
       "      <th>graphs</th>\n",
       "      <th>trees</th>\n",
       "      <th>dynamic programming</th>\n",
       "      <th>number theory</th>\n",
       "      <th>dsa_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>79</td>\n",
       "      <td>86</td>\n",
       "      <td>70</td>\n",
       "      <td>90</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83</td>\n",
       "      <td>78</td>\n",
       "      <td>72</td>\n",
       "      <td>70</td>\n",
       "      <td>80</td>\n",
       "      <td>69</td>\n",
       "      <td>75</td>\n",
       "      <td>70</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96</td>\n",
       "      <td>83</td>\n",
       "      <td>84</td>\n",
       "      <td>53</td>\n",
       "      <td>98</td>\n",
       "      <td>67</td>\n",
       "      <td>82</td>\n",
       "      <td>70</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98</td>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "      <td>64</td>\n",
       "      <td>84</td>\n",
       "      <td>82</td>\n",
       "      <td>85</td>\n",
       "      <td>81</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92</td>\n",
       "      <td>72</td>\n",
       "      <td>73</td>\n",
       "      <td>64</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>86</td>\n",
       "      <td>73</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   basics  stl  sorting  searching  graphs  trees  dynamic programming  \\\n",
       "0     100   79       86         70      90     73                   73   \n",
       "1      83   78       72         70      80     69                   75   \n",
       "2      96   83       84         53      98     67                   82   \n",
       "3      98   75       76         64      84     82                   85   \n",
       "4      92   72       73         64      90     70                   86   \n",
       "\n",
       "   number theory  dsa_score  \n",
       "0             73         53  \n",
       "1             70         62  \n",
       "2             70         59  \n",
       "3             81         65  \n",
       "4             73         75  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"dsa_scores_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['basics',\t'stl',\t'sorting',\t'searching',\t'graphs',\t'trees','dynamic programming',\t'number theory']]/100\n",
    "y = df['dsa_score']/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      basics   stl  sorting  searching  graphs  trees  dynamic programming  \\\n",
      "0       1.00  0.79     0.86       0.70    0.90   0.73                 0.73   \n",
      "1       0.83  0.78     0.72       0.70    0.80   0.69                 0.75   \n",
      "2       0.96  0.83     0.84       0.53    0.98   0.67                 0.82   \n",
      "3       0.98  0.75     0.76       0.64    0.84   0.82                 0.85   \n",
      "4       0.92  0.72     0.73       0.64    0.90   0.70                 0.86   \n",
      "...      ...   ...      ...        ...     ...    ...                  ...   \n",
      "1995    0.90  0.78     0.78       0.70    0.80   0.76                 0.70   \n",
      "1996    0.91  0.76     0.82       0.67    0.90   0.75                 0.81   \n",
      "1997    0.98  0.73     0.80       0.67    0.86   0.82                 0.80   \n",
      "1998    0.93  0.77     0.86       0.52    0.81   0.81                 0.77   \n",
      "1999    0.93  0.75     0.74       0.61    0.84   0.80                 0.78   \n",
      "\n",
      "      number theory  \n",
      "0              0.73  \n",
      "1              0.70  \n",
      "2              0.70  \n",
      "3              0.81  \n",
      "4              0.73  \n",
      "...             ...  \n",
      "1995           0.74  \n",
      "1996           0.79  \n",
      "1997           0.85  \n",
      "1998           0.65  \n",
      "1999           0.69  \n",
      "\n",
      "[2000 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.53\n",
      "1       0.62\n",
      "2       0.59\n",
      "3       0.65\n",
      "4       0.75\n",
      "        ... \n",
      "1995    0.61\n",
      "1996    0.75\n",
      "1997    0.73\n",
      "1998    0.56\n",
      "1999    0.51\n",
      "Name: dsa_score, Length: 2000, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      basics   stl  sorting  searching  graphs  trees  dynamic programming  \\\n",
      "1025    0.82  0.80     0.80       0.50    0.85   0.76                 0.79   \n",
      "1208    0.93  0.66     0.86       0.66    0.81   0.69                 0.82   \n",
      "1055    0.97  0.65     0.76       0.67    0.95   0.68                 0.82   \n",
      "367     0.96  0.82     0.86       0.65    0.88   0.74                 0.81   \n",
      "815     0.90  0.83     0.77       0.67    0.99   0.80                 0.74   \n",
      "...      ...   ...      ...        ...     ...    ...                  ...   \n",
      "1718    0.87  0.67     0.90       0.64    0.83   0.66                 0.69   \n",
      "1687    0.90  0.76     0.73       0.51    0.88   0.74                 0.79   \n",
      "210     0.81  0.78     0.74       0.70    0.98   0.75                 0.87   \n",
      "376     0.95  0.75     0.90       0.59    0.90   0.76                 0.74   \n",
      "1251    0.88  0.67     0.89       0.61    0.90   0.80                 0.73   \n",
      "\n",
      "      number theory  \n",
      "1025           0.66  \n",
      "1208           0.80  \n",
      "1055           0.72  \n",
      "367            0.80  \n",
      "815            0.67  \n",
      "...             ...  \n",
      "1718           0.66  \n",
      "1687           0.75  \n",
      "210            0.74  \n",
      "376            0.79  \n",
      "1251           0.70  \n",
      "\n",
      "[600 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1025    0.60\n",
      "1208    0.50\n",
      "1055    0.65\n",
      "367     0.70\n",
      "815     0.62\n",
      "        ... \n",
      "1718    0.59\n",
      "1687    0.73\n",
      "210     0.75\n",
      "376     0.67\n",
      "1251    0.70\n",
      "Name: dsa_score, Length: 600, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= model.predict(X_test)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00607319 0.00622388 0.00634549 0.00653595 0.00638335 0.00678295\n",
      " 0.00595957 0.00586713 0.00606702 0.00582851 0.00663675 0.00651381\n",
      " 0.00658592 0.00646282 0.00651335 0.00597572 0.006684   0.00621246\n",
      " 0.00655994 0.00659321 0.00699066 0.00596031 0.00666024 0.00661948\n",
      " 0.0064561  0.00689184 0.00661954 0.00604911 0.00639002 0.00673266\n",
      " 0.00641498 0.0060819  0.00677004 0.00639293 0.00668966 0.00681112\n",
      " 0.0066399  0.00640751 0.00587718 0.00681835 0.00605605 0.00681752\n",
      " 0.00637553 0.00694621 0.00643595 0.00572972 0.00665016 0.00624625\n",
      " 0.00701027 0.00639199 0.00648621 0.00635553 0.00691993 0.00654031\n",
      " 0.00631493 0.00685529 0.00694276 0.00611928 0.00643834 0.00645507\n",
      " 0.00594679 0.00613922 0.0065053  0.00637893 0.00673469 0.0067422\n",
      " 0.00662189 0.00611043 0.00674516 0.00582216 0.00577154 0.00595987\n",
      " 0.00565976 0.00602108 0.00672392 0.00706176 0.00642728 0.00615711\n",
      " 0.00671685 0.0068737  0.00618558 0.00671231 0.00604147 0.00550034\n",
      " 0.00661516 0.00623225 0.00644869 0.00649967 0.00632037 0.00650688\n",
      " 0.00666569 0.00634146 0.00653463 0.00631359 0.00608159 0.0060582\n",
      " 0.00627628 0.00615023 0.00632481 0.00663545 0.00679595 0.00610088\n",
      " 0.00649956 0.0063528  0.0061825  0.00596955 0.00603838 0.00724054\n",
      " 0.00633682 0.00622118 0.00605874 0.00703722 0.0063522  0.00656766\n",
      " 0.00601269 0.00664906 0.00686934 0.0070953  0.00582021 0.00636062\n",
      " 0.00658018 0.00652674 0.00612433 0.00627432 0.00631841 0.00630781\n",
      " 0.00605982 0.00637947 0.00647974 0.00646578 0.00692194 0.00597696\n",
      " 0.00601343 0.00606483 0.00595767 0.00649405 0.00595707 0.0067009\n",
      " 0.00614522 0.0066498  0.00622947 0.0060957  0.0059961  0.00731623\n",
      " 0.00590626 0.0057932  0.00659828 0.00716195 0.00640491 0.00617834\n",
      " 0.0070575  0.00624768 0.00659673 0.0062795  0.00646297 0.00679411\n",
      " 0.0065907  0.00682847 0.00631438 0.00610613 0.0059686  0.00625431\n",
      " 0.00654724 0.00584099 0.00613179 0.00652614 0.00705348 0.0064709\n",
      " 0.00662748 0.00626777 0.00628114 0.00601917 0.00638221 0.00667582\n",
      " 0.00641659 0.00668995 0.00627895 0.00674597 0.00603171 0.00589091\n",
      " 0.00583246 0.00689564 0.00579246 0.0065606  0.00653728 0.0062357\n",
      " 0.00684847 0.00609536 0.00576488 0.00654373 0.00613373 0.00664028\n",
      " 0.00644753 0.00614861 0.00639561 0.00634382 0.00657159 0.00612266\n",
      " 0.00630497 0.00660003 0.00681532 0.00630123 0.00640166 0.00632052\n",
      " 0.0062061  0.00580359 0.0061055  0.00684305 0.00577786 0.00627631\n",
      " 0.00661887 0.00627342 0.00643266 0.0062427  0.00607306 0.00635379\n",
      " 0.00644243 0.00595187 0.00588692 0.00662111 0.00669375 0.0060451\n",
      " 0.00657234 0.00654619 0.00616413 0.00604613 0.00690872 0.00617156\n",
      " 0.00605023 0.00583081 0.00570363 0.0057024  0.00706017 0.00635365\n",
      " 0.00565259 0.0063316  0.00712548 0.00586575 0.00605069 0.00640968\n",
      " 0.00613479 0.00602225 0.00659175 0.00635271 0.00685665 0.0068855\n",
      " 0.00651355 0.00680073 0.00670686 0.00621327 0.00663703 0.00641896\n",
      " 0.00636328 0.00648413 0.00657729 0.00718774 0.00595353 0.0063191\n",
      " 0.00642324 0.00634256 0.00693849 0.00635123 0.00637422 0.00651212\n",
      " 0.00662649 0.00610932 0.00591883 0.00628779 0.00686902 0.00655106\n",
      " 0.0063825  0.00623971 0.0059925  0.00713766 0.00619096 0.00651763\n",
      " 0.00598728 0.00640662 0.00629773 0.00638796 0.00681733 0.00602529\n",
      " 0.00664192 0.00607037 0.0068571  0.00603574 0.00627591 0.00638162\n",
      " 0.00655378 0.00643028 0.00674833 0.00646273 0.00626639 0.00632278\n",
      " 0.00575752 0.00601326 0.00608186 0.00605793 0.00582329 0.00582788\n",
      " 0.00620593 0.00643781 0.00638207 0.00614285 0.0070052  0.0063623\n",
      " 0.00679101 0.00632892 0.00628743 0.00598194 0.00650099 0.00632993\n",
      " 0.00662831 0.00695127 0.00660331 0.00607609 0.00616459 0.00676326\n",
      " 0.0066206  0.00650926 0.00628913 0.00618893 0.00615436 0.00644914\n",
      " 0.00627833 0.00646808 0.0067795  0.0061664  0.00666412 0.00620259\n",
      " 0.00656582 0.00655975 0.00643413 0.0062162  0.00652771 0.00639674\n",
      " 0.00654456 0.00623148 0.00646136 0.00603009 0.00615417 0.00653316\n",
      " 0.00634165 0.006117   0.00672435 0.00670687 0.00599184 0.00655618\n",
      " 0.00701353 0.00621087 0.00651934 0.00698361 0.00605155 0.00660959\n",
      " 0.00601213 0.00626503 0.00624703 0.00667955 0.00597705 0.00642729\n",
      " 0.00616398 0.00617206 0.00650053 0.00667557 0.00657839 0.00645128\n",
      " 0.00636786 0.0065043  0.00610168 0.00601461 0.00706567 0.00645208\n",
      " 0.00698931 0.00636643 0.00599456 0.00603284 0.00593002 0.00594121\n",
      " 0.007029   0.00649115 0.00681583 0.00653137 0.00614712 0.00605785\n",
      " 0.00571637 0.00644089 0.00635742 0.00599732 0.00669421 0.00667003\n",
      " 0.00613408 0.00671676 0.00642706 0.00672509 0.00618819 0.00666605\n",
      " 0.00639764 0.00665294 0.00600802 0.00662691 0.0060034  0.00660197\n",
      " 0.00589745 0.0068231  0.00627835 0.00643248 0.00691265 0.00695058\n",
      " 0.00672253 0.00660622 0.00616837 0.00638461 0.00699787 0.00667957\n",
      " 0.00662123 0.00603497 0.00698324 0.00616198 0.00603228 0.00672138\n",
      " 0.00613327 0.00670129 0.00567053 0.00632671 0.00631242 0.00645355\n",
      " 0.00712668 0.00646179 0.00675093 0.00621961 0.00597672 0.00649125\n",
      " 0.00641897 0.00698937 0.00658577 0.00659194 0.0063517  0.00616134\n",
      " 0.00639624 0.00572029 0.00642078 0.00660674 0.00614864 0.00631714\n",
      " 0.00663493 0.00689667 0.00660155 0.00689136 0.00616322 0.00572899\n",
      " 0.00612922 0.00672282 0.00610168 0.00675949 0.00645993 0.00617342\n",
      " 0.00681962 0.00646852 0.00691749 0.00626035 0.00658137 0.00605356\n",
      " 0.0059045  0.00634597 0.00641884 0.00621455 0.00626395 0.00663837\n",
      " 0.00679096 0.00630767 0.00660232 0.00640156 0.00600776 0.00643312\n",
      " 0.00607358 0.00671403 0.00667204 0.00611144 0.00595041 0.00635232\n",
      " 0.00627628 0.00672718 0.00604861 0.00645324 0.00575234 0.00625443\n",
      " 0.00633235 0.00682388 0.00656804 0.00647884 0.00653449 0.00656527\n",
      " 0.00646186 0.0057236  0.00645516 0.00696863 0.00632053 0.00624533\n",
      " 0.00617103 0.00684814 0.00606776 0.00616309 0.00649438 0.00635737\n",
      " 0.00621643 0.00637654 0.00660404 0.00608651 0.00640484 0.00631131\n",
      " 0.00586854 0.00666146 0.00638535 0.00686789 0.0066891  0.00641047\n",
      " 0.00651681 0.00562617 0.00584702 0.00643667 0.00660502 0.00656256\n",
      " 0.00680541 0.00631075 0.00706431 0.00658419 0.00684554 0.00704708\n",
      " 0.00631902 0.00652592 0.00569284 0.00637056 0.00645954 0.00613103\n",
      " 0.00662756 0.00684698 0.00634998 0.00657086 0.00610174 0.00626193\n",
      " 0.00679009 0.00699875 0.00622251 0.00635646 0.00655972 0.00633959\n",
      " 0.00646498 0.00635709 0.00706722 0.00597605 0.0068309  0.00627198\n",
      " 0.00606592 0.00648385 0.00649205 0.00644275 0.0065875  0.00673199\n",
      " 0.00629708 0.00616479 0.00627186 0.00587736 0.00628428 0.00699392\n",
      " 0.00637795 0.00672955 0.005912   0.00668901 0.00708305 0.00690208\n",
      " 0.00672467 0.00596546 0.00663657 0.00593853 0.00633992 0.00655297\n",
      " 0.00609291 0.00666739 0.00666067 0.00679161 0.00625268 0.00640499\n",
      " 0.0065401  0.00677734 0.00602087 0.00669182 0.00593906 0.00674112\n",
      " 0.00608767 0.00687692 0.00560987 0.0064987  0.0060386  0.00691608\n",
      " 0.00717759 0.00539118 0.006362   0.00678517 0.00640866 0.006203  ]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error value is: 0.4052861869322906\n"
     ]
    }
   ],
   "source": [
    "#mean_squared_error, r2_score\n",
    "print(\"mean squared error value is:\",mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score is: -69.68164677728882\n"
     ]
    }
   ],
   "source": [
    "print(\"r2 score is:\",r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, 'model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = joblib.load('model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "Predicted value: 56.70398694623253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chand\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "new_data = np.array([[83,78,72,70,80,69,75,70]])/100\n",
    "\n",
    "# Predict using the loaded model\n",
    "predictions = loaded_model.predict(new_data)\n",
    "\n",
    "# Print predictions\n",
    "print(\"Predictions:\")\n",
    "print(f\"Predicted value: {predictions[0]*100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
